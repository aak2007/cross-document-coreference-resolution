package edu.oregonstate.training;

import java.util.Map;
import java.util.Set;
import java.util.List;
import java.util.ArrayList;

import edu.stanford.nlp.dcoref.Mention;
import edu.oregonstate.CDCR;
import edu.oregonstate.CorefSystem;
import edu.oregonstate.classifier.LinearRegression;
import edu.oregonstate.features.Feature;
import edu.oregonstate.io.ResultOutput;
import edu.oregonstate.search.IterativeResolution;
import edu.stanford.nlp.dcoref.CorefCluster;
import edu.stanford.nlp.dcoref.Document;
import edu.stanford.nlp.stats.Counter;
import Jama.Matrix;

/**
 * train the linear regression Co-reference model
 * 
 * <p>
 * The specification of the whole process is shown in the algorithm 2 of the paper
 * 
 * @author Jun Xie (xie@eecs.oregonstate.edu)
 *
 */
public class Train {

	//TODO
	// clusters results, right now, I just use the original corpus
	// Later time, I will use clustering result using EM variant where the 
	// the initial points (and the number of clusters) are selected from the clusters
	// generated by a hierarchical agglomerative clustering algorithm using geometric heuristics
	private String[] mTopics;
	private int mEpoch;
	private double mCoefficient;
	private double mLamda;
	private String[] outputFileNames = {"one.csv", "two.csv", "three.csv", "four.csv", "five.csv", "six.csv", "seven.csv", "eight.csv", "nine.csv", "ten.csv", "initial.csv"};
	public static String currentOutputFileName = "";
	
	public Train( String[] topics, int epoch, double coefficient, double lamda) {
		mTopics = topics;
		mEpoch = epoch;
		mCoefficient = coefficient;
		mLamda = lamda;
		currentOutputFileName = CDCR.resultPath + outputFileNames[10];
	}
	
	/** main method for training the linear regression model, in the current time, use four roles now */
	public Matrix train(Matrix initialWeight) {
		Matrix model = initialWeight;
		// training data for linear regression
		for (int i = 0; i < mEpoch; i++) {
			ResultOutput.writeTextFile(CDCR.outputFileName, "Start train the model:"+ i +"th iteration ============================================================");
			currentOutputFileName = CDCR.resultPath + outputFileNames[i];
			/** all mentions in one doc cluster */
			for (String topic : mTopics) {
				ResultOutput.writeTextFile(CDCR.outputFileName, "Linear regreession begin to process topic " + topic+ "................");
				try {
					CorefSystem cs = new CorefSystem();
					Document document = cs.getDocument(topic);
					cs.corefSystem.coref(document);
				    
				    IterativeResolution ir = new IterativeResolution(document, cs.corefSystem.dictionaries(), model);
				    ir.merge(cs.corefSystem.dictionaries());
				}catch (Exception e) {
					e.printStackTrace();
					System.exit(1);
				}
				ResultOutput.writeTextFile(CDCR.outputFileName, "Linear regression end to process topic " + topic+ "................");
			}
			
			// <b>NOTE</b>: change this part in order to incorporate all 0 instances
			LinearRegression lr = new LinearRegression(currentOutputFileName, mCoefficient); 
			Matrix updateModel = lr.calculateWeight();
			
			Matrix coupdateModel = updateModel.times(1 - mLamda);
			Matrix comodel = model.times(mLamda);
			model = new Matrix(comodel.getRowDimension(), comodel.getColumnDimension());
			model = comodel.plus(coupdateModel);
			ResultOutput.writeTextFile(CDCR.outputFileName, "Finish train the model:"+ i +"th iteration ===================================================");
		}
		
		return model;
	}
	
	/**
	 * This procedure runs the high-precision sieves introduced just like the data generation loop in algorithm 2, creates training examples from 
	 * the clusters available after every merge operation. Since these deterministic models address only nominal clusters, at the end we generate 
	 * training data for events by inspecting all the pairs of singleton verbal clusters. Using this data, we train the initial linear regression model.
	 * 
	 * @return the initial weight set
	 */
	public Matrix assignInitialWeights() {
		ResultOutput.writeTextFile(CDCR.outputFileName, "Start train the initial model: ============================================================");
	
		// all mentions in one doc cluster
		for (String topic : mTopics) {
			ResultOutput.writeTextFile(CDCR.outputFileName, "begin to process topic " + topic+ "................");
			try {
				// generate training data for Entity
				CorefSystem cs = new CorefSystem();
			    Document document = cs.getDocument(topic);
			    cs.corefSystem.coref(document);
			    
			    // inspect all the pairs of singleton verbal clusters
			    Map<Integer, CorefCluster> corefClusters = document.corefClusters;
			    System.out.println("gold cluster: " + document.goldCorefClusters.size());
			    System.out.println(ResultOutput.printCluster(document.goldCorefClusters));
			    
			    System.out.println("coref cluster: " + corefClusters.size());
			    System.out.println(ResultOutput.printCluster(corefClusters));
			   
			    List<CorefCluster> verbSingletonCluster = new ArrayList<CorefCluster>();  // add the singleton verbal cluster
			    for (Integer clusterid : corefClusters.keySet()) {
			    	CorefCluster cluster = corefClusters.get(clusterid);
			    	Set<Mention> mentions = cluster.getCorefMentions();
			    	if (mentions.size() > 1) {
			    		continue;
			    	} else {
			    		for (Mention mention : mentions) {
			    			boolean isVerb = mention.isVerb;
			    			if (isVerb) {
				    			verbSingletonCluster.add(cluster);
				    			break;
			    			}
			    		}
			    	}
			    }
			    
			    // generate training data for Event
			    Map<Integer, Mention> goldMentions = document.allGoldMentions; // use the gold coref cluster to calculate the quality for this merge
			    for (int i = 0; i < verbSingletonCluster.size(); i++) {
			    	for (int j = 0; j < i; j++) {
			    		CorefCluster ci = verbSingletonCluster.get(i);
			    		CorefCluster cj = verbSingletonCluster.get(j);
			    		Counter<String> features = Feature.getFeatures(document, ci, cj, false, cs.corefSystem.dictionaries()); // get the feature
			    		Mention ciFirstMention = ci.getFirstMention();
			    		Mention cjFirstMention = cj.getFirstMention();
			    		double correct = 0.0;
			    		double total = 1.0;
			    		
			    		if (goldMentions.containsKey(ciFirstMention.mentionID) && goldMentions.containsKey(cjFirstMention.mentionID)) {
							if (goldMentions.get(ciFirstMention.mentionID).goldCorefClusterID == goldMentions.get(cjFirstMention.mentionID).goldCorefClusterID) {
								correct += 1.0;
							}
			    		}
			    		double quality = correct / total;
			    		String record = ResultOutput.buildString(features, quality);
			    		ResultOutput.writeTextFilewithoutNewline(currentOutputFileName, record);
			    	}
			    } 
			} catch (Exception e) {
				e.printStackTrace();
				System.exit(1);
			}
			ResultOutput.writeTextFile(CDCR.outputFileName, "end to process topic " + topic+ "................");
		}
		
		LinearRegression lr = new LinearRegression(currentOutputFileName, mCoefficient);
		Matrix initialModel = lr.calculateWeight();
		
		ResultOutput.writeTextFile(CDCR.outputFileName, "Finish train the initial model: ===================================================");
		return initialModel;
	}
	
}
