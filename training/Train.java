package edu.oregonstate.training;

import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.util.logging.Logger;
import java.util.Map;
import java.util.Set;
import java.util.List;
import java.util.ArrayList;

import edu.stanford.nlp.dcoref.Mention;
import edu.oregonstate.EventCoreference;
import edu.oregonstate.classifier.LinearRegression;
import edu.oregonstate.features.Feature;
import edu.oregonstate.search.IterativeResolution;
import edu.oregonstate.util.GlobalConstantVariables;
import edu.stanford.nlp.dcoref.CorefCluster;
import edu.stanford.nlp.dcoref.Document;
import edu.stanford.nlp.stats.Counter;
import Jama.Matrix;

/**
 * train the linear regression Co-reference model
 * 
 * <p>
 * The specification of the whole process is shown in the algorithm 2 of the paper
 * 
 * @author Jun Xie (xie@eecs.oregonstate.edu)
 *
 */
public class Train {
	public static final Logger logger = Logger.getLogger(EventCoreference.class.getName());
	
	// clusters results, right now, I just use the original corpus
	// Later time, I will use clustering result using EM variant where the 
	// the initial points (and the number of clusters) are selected from the clusters
	// generated by a hierarchical agglomerative clustering algorithm using geometric heuristics
	private String[] mTopics;
	private EventCoreference ec;
	private int mEpoch;
	private double mCoefficient;
	private double mLamda;
	private String[] outputFileNames = {"one.csv", "two.csv", "three.csv", "four.csv", "five.csv", "six.csv", "seven.csv", "eight.csv", "nine.csv", "ten.csv", "initial.csv"};
	public static String currentOutputFileName;
	
	public Train( EventCoreference ec, String[] topics, int epoch, double coefficient, double lamda) {
		this.ec = ec;
		mTopics = topics;
		mEpoch = epoch;
		mCoefficient = coefficient;
		mLamda = lamda;
		currentOutputFileName = GlobalConstantVariables.RESULT_PATH + outputFileNames[10];
	}
	
	// main method for training the linear regression model, in the current time, I use four roles now
	public Matrix train(Matrix initialWeight) {
		Matrix model = initialWeight;
		// training data for linear regression
		for (int i = 0; i < mEpoch; i++) {
			logger.info("Start train the model:"+ i +"th iteration ============================================================");
			currentOutputFileName = GlobalConstantVariables.RESULT_PATH + outputFileNames[i];
			// all mentions in one doc cluster
			for (String topic : mTopics) {
				System.out.println("begin to process topic 1................");
				try {
					Document document = ec.getDocument(topic);
				    ec.corefSystem.coref(document);
				    
				    IterativeResolution ir = new IterativeResolution(document, ec.corefSystem.dictionaries(), model);
				    ir.merge(ec.corefSystem.dictionaries());
				}catch (Exception e) {
					e.printStackTrace();
					System.exit(1);
				}
				System.out.println("end to process topic 1................");
			}
			
			LinearRegression lr = new LinearRegression(currentOutputFileName, mCoefficient);
			Matrix updateModel = lr.calculateWeight();
			
			Matrix coupdateModel = updateModel.times(1 - mLamda);
			Matrix comodel = model.times(mLamda);
			model = new Matrix(comodel.getRowDimension(), comodel.getColumnDimension());
			model = comodel.plus(coupdateModel);
			logger.info("Finish train the model:"+ i +"th iteration ===================================================");
		}
		
		return model;
	}
	
	/**
	 * This procedure runs the high-precision sieves introduced just like the data generation loop in algorithm 2, creates training examples from 
	 * the clusters available after every merge operation. Since these deterministic models address only nominal clusters, at the end we generate 
	 * training data for events by inspecting all the pairs of singleton verbal clusters. Using this data, we train the initial linear regression model.
	 * 
	 * @return the initial weight set
	 */
	public Matrix assignInitialWeights() {
		System.out.println("Start train the initial model: ============================================================");
	
		// all mentions in one doc cluster
		for (String topic : mTopics) {
			try {
				// generate training data for Entity
			    Document document = ec.getDocument(topic);
			    ec.corefSystem.coref(document);
			    
			    // inspect all the pairs of singleton verbal clusters
			    Map<Integer, CorefCluster> corefClusters = document.corefClusters;
			    List<CorefCluster> verbSingletonCluster = new ArrayList<CorefCluster>();  // add the singleton verbal cluster
			    for (Integer clusterid : corefClusters.keySet()) {
			    	CorefCluster cluster = corefClusters.get(clusterid);
			    	Set<Mention> mentions = cluster.getCorefMentions();
			    	if (mentions.size() > 1) {
			    		continue;
			    	} else {
			    		for (Mention mention : mentions) {
			    			boolean isVerb = mention.isVerb;
			    			if (isVerb) {
				    			verbSingletonCluster.add(cluster);
				    			break;
			    			}
			    		}
			    	}
			    }
			    
			    // generate training data for Event
			    Map<Integer, Mention> goldCorefClusters = document.allGoldMentions; // use the gold coref cluster to calculate the quality for this merge
			    for (int i = 0; i < verbSingletonCluster.size(); i++) {
			    	for (int j = 0; j < i; j++) {
			    		CorefCluster ci = verbSingletonCluster.get(i);
			    		CorefCluster cj = verbSingletonCluster.get(j);
			    		Counter<String> features = Feature.getFeatures(document, ci, cj, false, ec.corefSystem.dictionaries()); // get the feature
			    		Mention ciFirstMention = ci.getFirstMention();
			    		Mention cjFirstMention = cj.getFirstMention();
			    		double correct = 0.0;
			    		double total = 1.0;
			    		
			    		if (goldCorefClusters.containsKey(ciFirstMention.mentionID) && goldCorefClusters.containsKey(cjFirstMention.mentionID)) {
							if (goldCorefClusters.get(ciFirstMention.mentionID).goldCorefClusterID == goldCorefClusters.get(cjFirstMention.mentionID).goldCorefClusterID) {
								correct += 1.0;
							}
			    		}
			    		double quality = correct / total;
			    		String record = buildString(features, quality);
			    		writeTextFile(currentOutputFileName, record);
			    	}
			    } 
			} catch (Exception e) {
				e.printStackTrace();
				System.exit(1);
			}
			
			// wait for a while
			long start = System.currentTimeMillis();
			long end = start + 10*1000; // 60 seconds * 1000 ms/sec
			while (System.currentTimeMillis() < end)
			{

			}
		}
		
		LinearRegression lr = new LinearRegression(currentOutputFileName, mCoefficient);
		Matrix initialModel = lr.calculateWeight();
		
		System.out.println("Finish train the initial model: ===================================================");
		return initialModel;
	}

	// put features and quality together in order to create a string for output
	public static String buildString(Counter<String> features, double quality) {
		StringBuilder sb = new StringBuilder();
		boolean add = false;
		for (String feature : Feature.featuresName){
			double value = features.getCount(feature);
			if (value > 0.0) add = true;
			sb.append(value + ",");
		}
		sb.append(quality + "\n");
		if (add) {
			return sb.toString();
		} else {
			return "";
		}
	}
	
	// write the string to file
	public static void writeTextFile(String fileName, String s) {
	    try {
	    	BufferedWriter out = new BufferedWriter(new FileWriter(fileName, true));
	        out.write(s);
	        out.close();
	    } catch (IOException e) {
	      e.printStackTrace();
	    } 
	}
	
}
